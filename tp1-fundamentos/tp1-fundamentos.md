# Trabajo Práctico 1 - Fundamentos

## Ejercicio 1

### Fundamentos Filosóficos

¿Cómo funcionan las mentes? ¿Es posible que las máquinas actúen de forma inteligente como lo hacen las personas y, si lo hicieran, tendrían mentes reales y conscientes? ¿Cuáles son las implicaciones éticas de las máquinas inteligentes?

La afirmación de que las máquinas podrían actuar *como si* fueran inteligentes se denomina **hipótesis débil de la IA**, y la afirmación de que las máquinas que lo hacen *realmente* piensan (no sólo *simulan* pensar) se denomina **hipótesis fuerte de la IA**.

#### IA débil: ¿pueden las máquinas actuar con inteligencia?

La IA se fundó en el supuesto de que la IA débil es posible Se define la IA como la búsqueda del mejor programa agente en una arquitectura determinada. Con esta formulación, la IA es posible por definición: para cualquier arquitectura digital con k bits de almacenamiento de programas existen exactamente 2^k programas agentes, y todo lo que tenemos que hacer para encontrar el mejor es enumerarlos y probarlos todos

##### El argumento de la discapacidad

> La afirmación de que "una máquina nunca puede hacer X".

Las computadoras pueden hacer muchas cosas tan bien o mejor que los humanos, incluso cosas que la gente cree que requieren una gran perspicacia y comprensión humanas.
Las primeras conjeturas sobre los procesos mentales necesarios para producir un determinado comportamiento suelen ser erróneas. También es cierto, por supuesto, que hay muchas tareas en las que las computadoras aún no destacan, incluida la prueba de Turing.

##### La objeción matemática

> Las máquinas son sistemas formales que están limitados por el teorema de incompletitud -no pueden establecer la verdad de su propia sentencia Gödel mientras que los humanos no tienen esa limitación.

El teorema de incompletitud de Gödel sólo se aplica a los sistemas formales lo suficientemente potentes como para realizar operaciones aritméticas. Esto incluye las máquinas de Turing. Las máquinas de Turing son infinitas, mientras que las computadoras son finitos, por lo que cualquier computadora puede describirse como un sistema (muy grande) en lógica proposicional, que no está sujeto al teorema de incompletitud de Gödel. Los seres humanos se comportaron de forma inteligente durante miles de años antes de que inventaran las matemáticas, por lo que es poco probable que el razonamiento matemático formal desempeñe algo más que un papel periférico en lo que significa ser inteligente. Incluso si admitimos que las computadoras tienen limitaciones en lo que pueden demostrar, no hay pruebas de que los humanos sean inmunes a esas limitaciones.

##### El argumento de la informalidad
 
> La incapacidad de capturar todo en un conjunto de reglas lógicas se denomina **problema de cualificación**. Los agentes lógicos son vulnerables al problema de la cualificación

La postura que se dio en llamar "IA a la antigua", o GOFAI, sostiene que todo comportamiento inteligente puede ser capturado por un sistema que razona lógicamente a partir de un conjunto de hechos y reglas que describen el dominio. Por tanto, corresponde al agente lógico más simple. No se dirige contra las computadoras en sí, sino contra una forma concreta de programarlos.
Muchas de estas cuestiones, el problema de la cualificación, la incertidumbre, el aprendizaje, las formas compiladas de toma de decisiones, se han incorporado ya al diseño estándar de agentes inteligentes. Esto demuestra el progreso de la IA, no su imposibilidad.
Para entender cómo funcionan los agentes humanos (o de otros animales), tenemos que considerar el agente en su totalidad, no sólo el programa del agente. De hecho, el enfoque de la **cognición corporal** afirma que no tiene sentido considerar el cerebro por separado: la cognición tiene lugar dentro de un cuerpo, que está integrado en un entorno.

#### IA fuerte: ¿pueden las máquinas pensar de verdad?

Muchos filósofos han afirmado que una máquina que superara la **Prueba de Turing** seguiría sin pensar realmente, sino que sería sólo una simulación del pensamiento. Turing denomina argumento de la **conciencia** a esta afirmación de que la máquina tiene que ser consciente de sus propios estados mentales y acciones. Otro punto clave es la **fenomenología**, o el estudio de la experiencia directa: la máquina tiene que sentir emociones. Otros se centran en la **intencionalidad**, es decir, en la cuestión de si las supuestas creencias, deseos y otras representaciones de la máquina son realmente "sobre" algo en el mundo real.
¿Por qué deberíamos insistir en un estándar más alto para las máquinas que para los humanos? Al fin y al cabo, en la vida cotidiana nunca tenemos pruebas directas de los estados mentales internos de otros seres humanos.
La mayoría de la gente se siente cómoda diciendo que una simulación informática de la suma es suma, y del ajedrez es ajedrez. De hecho, solemos hablar de una implementación de la suma o del ajedrez, no de una simulación.
¿Cómo es que los humanos tienen mentes reales, no sólo cuerpos que generan procesos neurofisiológicos? Los esfuerzos filosóficos por resolver este **problema mente-cuerpo** están directamente relacionados con la cuestión de si las máquinas pueden tener mentes reales.
Concluir que la actividad mental del pensamiento (un proceso sin extensión espacial ni propiedades materiales) y los procesos físicos del cuerpo deben existir en reinos separados es lo que ahora llamaríamos una teoría **dualista**.
La teoría **monista** de la mente, a menudo llamada **fisicalismo**, evita este problema afirmando que la mente no está separada del cuerpo, que los estados mentales son estados físicos. El fisicalismo permite, al menos en principio, la posibilidad de una IA fuerte. El problema para los fisicalistas es explicar cómo los estados físicos -en particular, las configuraciones moleculares y los procesos electroquímicos del cerebro- pueden ser simultáneamente **estados mentales**.

##### Estados mentales y el cerebro en un tanque

Los **estados intencionales** son estados, como creer, saber, desear, temer, etc., que se refieren a algún aspecto del mundo exterior.
Si el fisicalismo es correcto, debe darse el caso de que la descripción adecuada del estado mental de una persona viene determinada por el estado cerebral de esa persona.
Imagínese que le extraen el cerebro al nacer y lo colocan en un tanque maravillosamente diseñado. El tanque alimenta su cerebro y le permite crecer y desarrollarse. Al mismo tiempo, su cerebro recibe señales electrónicas de una simulación informática de un mundo ficticio, y las señales motoras de su cerebro son interceptadas y utilizadas para modificar la simulación según convenga.
El contenido de los estados mentales puede interpretarse desde dos puntos de vista distintos. El punto de vista del "**contenido amplio**" lo interpreta desde el punto de vista de un observador externo omnisciente con acceso a toda la situación, que puede distinguir diferencias en el mundo. Según este punto de vista, el contenido de los estados mentales implica tanto el estado cerebral como la historia del entorno. En cambio, el **contenido estrecho** sólo tiene en cuenta el estado cerebral. El contenido estrecho de los estados cerebrales de un "come-hamburguesas" real y de un "come-hamburguesas" de cerebro-en-un-tanque es el mismo en ambos casos.
Si a uno le preocupa la cuestión de si los sistemas de IA realmente piensan y realmente tienen estados mentales, entonces el contenido restringido es apropiado; simplemente no tiene sentido decir que si un sistema de IA realmente piensa o no depende de condiciones externas a ese sistema.

##### El funcionalismo y el experimento de sustitución del cerebro

La teoría del **funcionalismo** dice que un estado mental es cualquier condición causal intermedia entre la entrada y la salida. Según la teoría funcionalista, dos sistemas cualesquiera con procesos causales isomórficos tendrían los mismos estados mentales. Por tanto, un programa informático podría tener los mismos estados mentales que una persona.
Supongamos que la neurofisiología se ha desarrollado hasta el punto de comprender perfectamente el comportamiento de entrada-salida y la conectividad de todas las neuronas del cerebro humano. Supongamos además que podemos construir dispositivos electrónicos microscópicos que imitan este comportamiento y pueden interconectarse sin problemas con el tejido neuronal. Por último, supongamos que alguna técnica quirúrgica milagrosa puede sustituir neuronas individuales por los dispositivos electrónicos correspondientes sin interrumpir el funcionamiento del cerebro en su conjunto. El experimento consiste en sustituir gradualmente todas las neuronas de la cabeza de alguien por dispositivos electrónicos.
Según la definición del experimento, el comportamientocomportamiento externo del sujeto debe permanecer inalterado en comparación con lo que se observaría si no se llevara a cabo la operación.
Por definición del experimento, el comportamiento externo del sujeto debe permanecer inalterado en comparación con lo que se observaría si no se llevara a cabo la operación.
Nótese que, para que el comportamiento externo permanezca igual mientras el sujeto se vuelve gradualmente inconsciente, debe darse el caso de que la volición del sujeto sea eliminada instantánea y totalmente; de lo contrario, la contracción de la consciencia se reflejaría en el comportamiento externo
Consideremos lo que ocurre si hacemos preguntas al sujeto sobre su experiencia consciente durante el periodo en el que no quedan neuronas reales. Según las condiciones del experimento, obtendremos respuestas como "me siento bien".
Entonces debemos tener una explicación de las manifestaciones de conciencia producidas por el cerebro electrónico que apele únicamente a las propiedades funcionales de las neuronas. Y esta explicación debe aplicarse también al cerebro real, que tiene las mismas propiedades funcionales. Hay tres conclusiones posibles:
1. Los mecanismos causales de la conciencia que generan este tipo de resultados en los cerebros normales siguen funcionando en la versión electrónica, que por tanto es consciente.
2. Los eventos mentales conscientes en el cerebro normal no tienen conexión causal con el comportamiento, y faltan en el cerebro electrónico, que por tanto no es consciente.
3. El experimento es imposible y, por tanto, las especulaciones al respecto carecen de sentido.
Aunque no podemos descartar la segunda posibilidad, reduce la conciencia a lo que los filósofos llaman un papel **epifenoménico**: algo que ocurre, pero que no proyecta ninguna sombra, por así decirlo, sobre el mundo observable. Además, si la conciencia es realmente epifenoménica, no puede ser que el sujeto diga "¡Ay!" porque le duele, es decir, por la experiencia consciente del dolor. En su lugar, el cerebro debe contener un segundo mecanismo inconsciente
responsable del "Ay".

##### El naturalismo biológico y la Habitación China

El **naturalismo biológico** ha planteado un fuerte desafío al funcionalismo, según el cual los estados mentales son características emergentes de alto nivel causadas por procesos físicos de bajo nivel en las neuronas, y lo que importa son las propiedades (no especificadas) de las neuronas. Así pues, los estados mentales no pueden duplicarse simplemente sobre la base de un programa que tenga la misma estructura funcional con el mismo comportamiento de entrada-salida; requeriríamos que el programa se ejecutara en una arquitectura con el mismo poder causal que las neuronas.

Un sistema hipotético que consiste en un humano, que sólo entiende inglés, equipado con un libro de reglas, escrito en inglés, y varios montones de papel, algunos en blanco, otros con inscripciones indescifrables. El sistema está dentro de una habitación con una pequeña abertura al exterior. Por la abertura aparecen trozos de papel con símbolos indescifrables. El humano encuentra los símbolos correspondientes en el libro de reglas y sigue las instrucciones. Las instrucciones pueden incluir escribir símbolos en nuevas tiras de papel, encontrar símbolos en las pilas, reorganizar las pilas, etcétera. Al final, las instrucciones harán que uno o varios símbolos se transcriban en un trozo de papel que se devuelve al mundo exterior.
Desde fuera, vemos un sistema que recibe datos en forma de frases en chino y genera respuestas en chino tan "inteligentes" como las de la conversación imaginada por Turing. La persona que está en la sala no entiende chino (dado). El libro de reglas y las pilas de papel, al ser sólo trozos de papel, no entienden chino. Por lo tanto, no hay comprensión del chino. Por lo tanto, ejecutar el programa adecuado no genera necesariamente comprensión.

Preguntar si el ser humano en la sala entiende chino es análogo a preguntar si la CPU puede tomar raíces cúbicas. En ambos casos, la respuesta es no, y en ambos casos, según la respuesta del sistema, todo el sistema tiene la capacidad en cuestión.

##### Conciencia, qualia y la brecha explicativa

La **conciencia** suele dividirse en aspectos como la comprensión y la autoconciencia. El aspecto en el que nos centraremos es el de la experiencia subjetiva: por qué se siente algo al tener ciertos estados cerebrales mientras que presumiblemente no se siente nada al tener otros estados físicos. El término técnico para la naturaleza intrínseca de las experiencias es **qualia**. Los qualia son un reto no sólo para el funcionalismo, sino para toda la ciencia. Sencillamente, no existe ninguna forma de razonamiento aceptada actualmente que permita concluir que la entidad propietaria de esas neuronas tiene una experiencia subjetiva concreta. Esta **brecha explicativa** ha llevado a algunos filósofos a concluir que los seres humanos son sencillamente incapaces de formarse una idea adecuada de su propia conciencia.

#### Ética y riesgos del desarrollo de la inteligencia artificial

Si es más probable que los efectos de la tecnología de IA sean negativos que positivos, entonces sería responsabilidad moral de los trabajadores del sector reorientar sus investigaciones. Todos los científicos e ingenieros se enfrentan a consideraciones éticas sobre cómo deben actuar en el trabajo, qué proyectos deben realizarse o no y cómo deben gestionarse. La IA, sin embargo, parece plantear algunos problemas nuevos más allá de, por ejemplo, construir puentes que no se caigan:
- La gente podría perder su trabajo a causa de la automatización.
- La gente podría tener demasiado (o demasiado poco) tiempo libre.
- La gente podría perder su sentido de ser única.
- Los sistemas de IA podrían utilizarse para fines no deseados.
- El uso de los sistemas de IA podría provocar una pérdida de responsabilidad.
- El éxito de la IA podría significar el fin de la raza humana.

Hasta ahora, la automatización mediante las tecnologías de la información en general y la IA en particular ha creado más puestos de trabajo de los que ha eliminado, y ha creado empleos más interesantes y mejor pagados. Puede que acabemos en un futuro en el que el desempleo sea elevado, pero incluso los desempleados sirvan como gestores de su propio cuadro de trabajadores robotizados.
En una economía de la información marcada por la comunicación de banda ancha y la fácil reproducción de la propiedad intelectual, ser ligeramente mejor que la competencia tiene una gran recompensa. Así que cada vez hay más presión para que todo el mundo trabaje más. La IA aumenta el ritmo de la innovación tecnológica y, por tanto, contribuye a esta tendencia general, pero también promete permitirnos tomarnos un tiempo libre y dejar que nuestros agentes automatizados se ocupen de todo durante un tiempo.
La humanidad ha sobrevivido a otros reveses a nuestro sentido de la unicidad: alejar la Tierra del centro del sistema solar y situar al Homo sapiens al mismo nivel que otras especies. La IA, si tiene un éxito generalizado, puede ser al menos tan amenazadora para los supuestos morales de la sociedad del siglo XXI como lo fue la teoría de la evolución de Darwin para los del siglo XIX.
La posesión de potentes robots puede dar a una nación un exceso de confianza, provocando que vaya a la guerra de forma más imprudente de lo necesario. En la mayoría de las guerras, al menos una de las partes tiene un exceso de confianza en sus capacidades militares; de lo contrario, el conflicto se habría resuelto pacíficamente. La IA tiene el potencial de producir vigilancia masiva, la informatización conduce a una pérdida de privacidad.
Al diseñar sistemas expertos como agentes, hay que pensar que las acciones influyen en el comportamiento del profesional. Si los sistemas expertos llegan a ser fiablemente más precisos que los profesionales humanos, éstos podrían llegar a ser legalmente responsables si no utilizan las recomendaciones de un sistema experto.
Si las transacciones monetarias son realizadas "en nombre propio" por un agente inteligente, ¿es uno responsable de las deudas contraídas? ¿Sería posible que un agente inteligente tuviera activos por sí mismo y realizara operaciones electrónicas en su propio nombre? Hasta ahora, estas cuestiones no parecen estar bien entendidas.
La ley aún tiene que ponerse al día con los nuevos avances.
Especificar la función de utilidad correcta para que un sistema de IA la maximice no es tan fácil, tenemos que tener mucho cuidado con lo que pedimos, mientras que los humanos no tendrían ningún problema en darse cuenta de que la función de utilidad propuesta no puede tomarse al pie de la letra. Los humanos tienen algunas tendencias agresivas innatas, debido a la selección natural. Las máquinas que construimos no tienen por qué ser innatamente agresivas, a menos que decidamos construirlas así.
La función de aprendizaje del sistema de IA puede hacer que evolucione hacia un sistema con un comportamiento no deseado.
La "explosión de inteligencia" también se ha denominado **singularidad tecnológica**. El concepto de máquinas ultrainteligentes parte de la base de que la inteligencia es un atributo especialmente importante y que, si se tiene suficiente, todos los problemas pueden resolverse. Pero sabemos que la computabilidad y la complejidad computacional tienen límites.
Existe incluso una nueva palabra -**transhumanismo**- para el activo movimiento social que espera este futuro en el que los humanos se fusionen con -o sean sustituidos por- inventos robóticos y biotecnológicos.
¿Cómo diseñar una **IA amigable**? La amabilidad (el deseo de no hacer daño a los humanos) debe estar presente desde el principio, pero los diseñadores deben reconocer que sus propios diseños pueden tener fallos y que el robot aprenderá y evolucionará con el tiempo.
aprenderá y evolucionará con el tiempo. Así pues, el reto consiste en diseñar un mecanismo para que los sistemas de IA evolucionen en el marco de un sistema de controles y equilibrios, y en dotar a los sistemas de funciones de utilidad que sigan siendo amistosas ante tales cambios.
Por último, consideremos el punto de vista del robot. Si los robots adquieren conciencia, tratarlos como meras "máquinas" (por ejemplo, desmontarlos) podría ser inmoral.

## Ejercicio 3

El artículo hace los siguientes comentarios:

### Dehumanización:

Bender: La utilización de modelos de lenguaje generativos puede llevar a una falta de percepción de la humanidad real, al blurring de las líneas entre seres humanos y máquinas, lo cual contribuye a la dehumanización.

### Impacto Ético y Moral:

Blake Lemoine: El uso de chatbots en objetos como muñecas sexuales puede llevar a una habituación de las personas a tratar a entidades que parecen humanas como si no lo fueran, lo cual plantea serias cuestiones éticas sobre el tratamiento de estos sistemas.

### Problemas con el Concepto de Significado:

Bender: Los LLMs pueden perpetuar problemas lingüísticos y culturales sin tener un entendimiento real del significado, lo cual es problemático porque los modelos procesan lenguaje de manera probabilística, sin referencia a la realidad o al significado profundo.

### Narcisismo Tecnológico:

Judith Butler: La idea de que la tecnología puede lograr lo que se considera distintivamente humano o mejorar la humanidad puede llevar a una forma de narcisismo tecnológico, donde se busca demostrar que las máquinas pueden superar a los humanos en todas las capacidades.

### Creación de "Personas Falsas":

Daniel Dennett: La creación de "personas falsas" o entidades artificiales que imitan a los humanos plantea riesgos serios, ya que estas máquinas no tienen la capacidad de experimentar realmente la humanidad, lo cual puede llevar a una falta de responsabilidad y a la creación de "armas" que amenazan la estabilidad social.

### Riesgos de Confusión y Mal Uso:

Bender: La confusión y el mal uso potencial de los LLMs pueden resultar en una erosión de las barreras entre lo que es verdaderamente humano y lo que es artificial, lo cual puede tener consecuencias peligrosas para la sociedad.

### Defensa

Desde mí perspectiva no estoy de acuerdo con la mayoría del artículo.

Humanizar cosas no humanas es algo que hemos hecho mucho antes de que existieran las computadoras. Les damos nombres propios a nuestras mascotas, les hablamos como si nos entendieran, etc. Lo considero un comportamiento usual, el artículo no proporciona ningún fundamento para creer que esto contribuye a deshumanizar a otras personas, y no conozco ninguno.

Por otro lado, el punto de vista de Lemoine casi inverso al anterior, tampoco ofrece ningún fundamento. La deshumanización ha existido siempre y es anterior a las computadoras. Aunque actualmente la esclavitud y prácticas similares son ilegales en países occidentales, como sociedad debemos buscar combatirlas, y argumentos vagos y vacíos como este no contribuyen.

El punto de vista de Bender me parece válido. Los LLMs no representan a todas las personas por igual. Lo que puedan generar depende de los datos con los que fueron entrenados. Y si eventualmente se quieren utilizar para crear máquinas iguales a las personas, ese concepto de "persona" estaría sesgado.

La opinión de Judith Butler me parece puramente subjetivo. Ella hace un juicio de valor acerca de los objetivos que pueda tener la sociedad. No es discutible.

Estoy de acuerdo con la idea de Daniel Dennett y Bender. Pero como sociedad estamos ante la disyuntiva de desarrollar nuevas herramientas que puedan ser mal utilizadas o buscar la ignorancia.
